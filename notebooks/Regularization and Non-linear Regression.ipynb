{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization of Regression Functions\n",
    "\n",
    "We have seen how we can use model selection approaches for selecting the best model from a set of candidate models, using information-theoretic model selection criteria. This approach is great of you have a small number of pre-specified models to choose from, or if you are trying to evaluate a particular covariate, as an alternative to statistical hypothesis testing. \n",
    "\n",
    "In some scenarios, there is no pre-specified suite of models to compare, but rather there is a set of candidate covariates, which can be combined in various ways to generate candidate models. If this set is very large, the number of models becomes too large to feasibly analyze using model selection critera such as AIC.\n",
    "\n",
    "An alternative is to partially automate the business of variable selection, using regularization. This involves including a penalty term to the model for the values of the estimated coefficients that has the effect of **shrinking** the coefficient values toward zero unless the coefficients can offset the penalty by increasing the likelihood of the model.\n",
    "\n",
    "Another motivation for using regularization is "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
